Export the html data function and the score calculation function.

Used packages:: axios, cheerio & robots.txt parser.



# Robots.txt has been parsed and there are functions that extract the rules, categories of rules and filters out the rules based off a category.








# Create a page structure that holds the html data, url, and links 
# Serialisation and deserialisation will be done via a json file.

Page {
    title,
    url,
    html_data,
    links,
    score,
}

-> score is the finalised score it has based off the query.

Work on implementing checks for robots.txt at the root directory of a website.

Example:
https://webscraper.io/robots.txt

even better, work on reverse crawling to the root directory so that we can get the robots.txt from there.